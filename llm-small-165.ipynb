{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom tokenizers import ByteLevelBPETokenizer\nfrom tqdm import tqdm\nfrom datasets import load_dataset","metadata":{"_uuid":"5718ffc2-d901-47e7-86ae-29ad149ca1e4","_cell_guid":"f87af8a6-c600-476d-97c8-6ecd69b008a3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-15T11:58:59.031203Z","iopub.execute_input":"2025-09-15T11:58:59.031730Z","iopub.status.idle":"2025-09-15T11:59:04.921942Z","shell.execute_reply.started":"2025-09-15T11:58:59.031702Z","shell.execute_reply":"2025-09-15T11:59:04.921190Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%env CUDA_LAUNCH_BLOCKING=1","metadata":{"_uuid":"b5b0b5c6-c5db-4895-a0c5-7a7145ca5f63","_cell_guid":"b895d73b-1456-45db-aac7-6c8970717289","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-10T05:46:49.729386Z","iopub.execute_input":"2025-09-10T05:46:49.729636Z","iopub.status.idle":"2025-09-10T05:46:49.734447Z","shell.execute_reply.started":"2025-09-10T05:46:49.729619Z","shell.execute_reply":"2025-09-10T05:46:49.733536Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GPTConfig:\n    def __init__(self, vocab_size=40000, d_model=512, n_heads=8, n_layers=24, d_ff=4096, max_len=512, dropout=0.05):\n        self.vocab_size = vocab_size\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.n_layers = n_layers\n        self.d_ff = d_ff\n        self.max_len = max_len\n        self.dropout = dropout\n\nCONFIG = GPTConfig()\nSEQ_LEN = CONFIG.max_len\nBATCH_SIZE = 8\nGRAD_ACCUM = 5\nEPOCHS = 3\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nCHECKPOINT_DIR = './checkpoints'\nCHECKPOINT_STEPS = 12000\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)","metadata":{"_uuid":"77bea9dd-ffff-4f24-863d-9b22c9c56ffb","_cell_guid":"d90010ca-b4ac-4bea-b623-17976d71c367","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-15T11:59:19.801629Z","iopub.execute_input":"2025-09-15T11:59:19.802475Z","iopub.status.idle":"2025-09-15T11:59:19.887083Z","shell.execute_reply.started":"2025-09-15T11:59:19.802435Z","shell.execute_reply":"2025-09-15T11:59:19.886244Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not os.path.exists('./tokenizer/vocab.json'):\n    dataset = load_dataset('wikitext', 'wikitext-103-raw-v1', split='train')\n    with open('wikitext103_train.txt','w',encoding='utf-8') as f:\n        for line in dataset['text']:\n            f.write(line + '\\n')\n\n\n    tokenizer = ByteLevelBPETokenizer()\n    tokenizer.train(files=['wikitext103_train.txt'], vocab_size=CONFIG.vocab_size, min_frequency=2,\n                    special_tokens=[\"<s>\",\"<pad>\",\"</s>\",\"<unk>\",\"<mask>\"])\n    os.makedirs('./tokenizer', exist_ok=True)\n    tokenizer.save_model('./tokenizer')\nelse:\n    tokenizer = ByteLevelBPETokenizer('./tokenizer/vocab.json', './tokenizer/merges.txt')","metadata":{"_uuid":"c1db8c46-58e4-4be6-acf7-367a1a007289","_cell_guid":"d22430f5-6d7a-431a-9111-7a2e149d635d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-15T11:59:22.124131Z","iopub.execute_input":"2025-09-15T11:59:22.124399Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self, dataset_split, tokenizer, seq_len=512):\n        self.examples = []\n        self.seq_len = seq_len\n        for text in dataset_split['text']:\n            ids = tokenizer.encode(text).ids\n            stride = seq_len // 2\n            for i in range(0, len(ids), stride):\n                chunk = ids[i:i+seq_len]\n                if len(chunk) < 2:\n                    continue\n                self.examples.append(chunk)\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, idx):\n        chunk = self.examples[idx]\n        input_ids = chunk[:-1]\n        labels = chunk[1:]\n        if len(input_ids) < self.seq_len - 1:\n            pad_len = (self.seq_len -1) - len(input_ids)\n            input_ids = input_ids + [0]*pad_len\n            labels = labels + [-100]*pad_len\n\n        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(labels, dtype=torch.long)","metadata":{"_uuid":"5e36735e-54f0-40d3-b0ee-c69ae6ace0fd","_cell_guid":"cb81d738-cdea-4f71-803b-5438a39812c3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-10T05:42:10.539166Z","iopub.execute_input":"2025-09-10T05:42:10.539688Z","iopub.status.idle":"2025-09-10T05:42:10.545804Z","shell.execute_reply.started":"2025-09-10T05:42:10.539666Z","shell.execute_reply":"2025-09-10T05:42:10.545042Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_batch(batch):\n    x = torch.stack([b[0] for b in batch], dim=0)\n    y = torch.stack([b[1] for b in batch], dim=0)\n    return x, y","metadata":{"_uuid":"237bdaab-ca71-4b88-87d4-65aa22ab2453","_cell_guid":"4cecea80-148c-4054-9f22-5e282b143490","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-10T05:42:14.845814Z","iopub.execute_input":"2025-09-10T05:42:14.846531Z","iopub.status.idle":"2025-09-10T05:42:14.850219Z","shell.execute_reply.started":"2025-09-10T05:42:14.846508Z","shell.execute_reply":"2025-09-10T05:42:14.849480Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\nfrom torch.utils.data import DataLoader\n\ntrain_split = load_dataset('wikitext', 'wikitext-103-raw-v1', split='train[:650000]')\n\nval_split = load_dataset('wikitext', 'wikitext-103-raw-v1', split='validation[:20000]')\n\ntrain_ds = TextDataset(train_split, tokenizer, seq_len=SEQ_LEN)\nval_ds   = TextDataset(val_split, tokenizer, seq_len=SEQ_LEN)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    drop_last=True,\n    collate_fn=collate_batch\n)\n\nval_loader = DataLoader(\n    val_ds,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    drop_last=True,\n    collate_fn=collate_batch\n)\n\nprint(f\"Train samples: {len(train_ds)}\")\nprint(f\"Val samples: {len(val_ds)}\")","metadata":{"_uuid":"7466fe7d-0c05-482a-a3bd-37b0ee83cf08","_cell_guid":"f79b6e11-49ec-4768-8b82-9d6d33168768","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-10T05:42:18.864757Z","iopub.execute_input":"2025-09-10T05:42:18.865360Z","iopub.status.idle":"2025-09-10T05:44:22.180118Z","shell.execute_reply.started":"2025-09-10T05:42:18.865336Z","shell.execute_reply":"2025-09-10T05:44:22.179362Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###################\ntrain_split = load_dataset('wikitext', 'wikitext-103-raw-v1', split='train[:75000]')\nval_split = load_dataset('wikitext', 'wikitext-103-raw-v1', split='validation[:5000]')\nval_ds = TextDataset(val_split, tokenizer, seq_len=SEQ_LEN)\ntrain_ds = TextDataset(dataset, tokenizer, seq_len = SEQ_LEN)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn = collate_batch)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)","metadata":{"_uuid":"ca8ac18b-225e-484c-87cc-40c70ced4d6c","_cell_guid":"065d3745-ebf5-4447-b6e5-3b33784aff2d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-08T11:25:23.229184Z","iopub.execute_input":"2025-09-08T11:25:23.229483Z","iopub.status.idle":"2025-09-08T11:32:00.018107Z","shell.execute_reply.started":"2025-09-08T11:25:23.229461Z","shell.execute_reply":"2025-09-08T11:32:00.017501Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CausalSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        assert config.d_model % config.n_heads == 0\n        self.n_heads = config.n_heads\n        self.head_dim = config.d_model // config.n_heads\n        self.qkv_proj = nn.Linear(config.d_model, 3*config.d_model)\n        self.out_proj = nn.Linear(config.d_model, config.d_model)\n        self.dropout = nn.Dropout(config.dropout)\n        self.scale = 1/math.sqrt(self.head_dim)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        qkv = self.qkv_proj(x).view(B,T,3,self.n_heads, self.head_dim).permute(2,0,3,1,4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        attn = torch.matmul(q,k.transpose(-2, -1))*self.scale\n        mask = torch.tril(torch.ones(T, T,device=x.device)).unsqueeze(0).unsqueeze(0)\n        attn = attn.masked_fill(mask==0, -torch.inf)\n        attn = torch.softmax(attn, dim=-1)\n        attn = self.dropout(attn)\n        out = torch.matmul(attn, v).transpose(1, 2).contiguous().view(B, T, C)\n        return self.out_proj(out)","metadata":{"_uuid":"d09cd640-7dd5-4895-8040-f7a1b16d1e53","_cell_guid":"31d07c92-0f9e-4dc8-a43a-19c57d199c38","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-10T05:44:27.835120Z","iopub.execute_input":"2025-09-10T05:44:27.835440Z","iopub.status.idle":"2025-09-10T05:44:27.842972Z","shell.execute_reply.started":"2025-09-10T05:44:27.835421Z","shell.execute_reply":"2025-09-10T05:44:27.842211Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(config.d_model)\n        self.attn = CausalSelfAttention(config)\n        self.ln2 = nn.LayerNorm(config.d_model)\n        self.ff = nn.Sequential(\n            nn.Linear(config.d_model, config.d_ff),\n            nn.GELU(),\n            nn.Linear(config.d_ff, config.d_model),\n            nn.Dropout(config.dropout)\n        )\n\n    def forward(self, x):\n        x = x + self.attn(self.ln1(x))\n        x = x + self.ff(self.ln2(x))\n        return x","metadata":{"_uuid":"b4a0f6ca-e151-4639-99b5-34ff3dd9bbc3","_cell_guid":"0ff0562c-83ef-4b84-8304-dc1becce0054","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-10T05:44:31.931657Z","iopub.execute_input":"2025-09-10T05:44:31.931926Z","iopub.status.idle":"2025-09-10T05:44:31.937088Z","shell.execute_reply.started":"2025-09-10T05:44:31.931905Z","shell.execute_reply":"2025-09-10T05:44:31.936516Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.tok_emb = nn.Embedding(config.vocab_size, config.d_model)\n        self.pos_emb = nn.Embedding(config.max_len, config.d_model)\n        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(config.n_layers)])\n        self.ln_f = nn.LayerNorm(config.d_model)\n        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n\n    def forward(self, idx):\n        B, T = idx.shape\n        tok = self.tok_emb(idx)\n        pos = self.pos_emb(torch.arange(T, device=idx.device)).unsqueeze(0).expand(B, -1, -1)\n        x = tok+pos\n        for block in self.blocks:\n            x = block(x)\n        x = self.ln_f(x)\n        return self.head(x)\n\nmodel = GPT(CONFIG)\nprint(f\"trainabale model params: {sum(p.numel() for p in model.parameters() if p.requires_grad)//1e6:.1f}M\")\nprint(f\"total model params: {sum(p.numel() for p in model.parameters())//1e6:.1f}M\")","metadata":{"_uuid":"929faf34-e6a5-4d52-be8b-1d39c5a1451f","_cell_guid":"26fa3cc6-1d53-434e-8a14-d2503fda63f1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-10T05:47:24.147041Z","iopub.execute_input":"2025-09-10T05:47:24.147757Z","iopub.status.idle":"2025-09-10T05:47:25.543936Z","shell.execute_reply.started":"2025-09-10T05:47:24.147733Z","shell.execute_reply":"2025-09-10T05:47:25.543277Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=2.5e-4)\nscaler = torch.cuda.amp.GradScaler()\n\nfor epoch in range(EPOCHS):\n    model.train()\n    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n    optimizer.zero_grad()\n    for step, (x,y) in enumerate(pbar):\n        x,y = x.to(DEVICE), y.to(DEVICE)\n        with torch.cuda.amp.autocast():\n            logits = model(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-100)/GRAD_ACCUM\n        scaler.scale(loss).backward()\n        if(step+1)%GRAD_ACCUM==0:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm(model.parameters(), 1, 0)\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            pbar.set_postfix({'loss':loss.item()*GRAD_ACCUM})\n\n        if (step+1) % CHECKPOINT_STEPS == 0:\n            ckpt_path = os.path.join(CHECKPOINT_DIR, f'gpt_epoch{epoch}_step{step+1}.pt')\n            torch.save({'model_state': model.state_dict(), 'optimizer_state': optimizer.state_dict(), 'epoch': epoch, 'step': step+1}, ckpt_path)\n            print(f'Checkpoint saved at step {step+1}: {ckpt_path}')\n\n\n    ckpt_path = os.path.join(CHECKPOINT_DIR, f'gpt_epoch{epoch}.pt')\n    torch.save({'model_state':model.state_dict(), 'optimizer_state':optimizer.state_dict(), 'epoch':epoch}, ckpt_path)\n    print(f'Checkpoint saved: {ckpt_path}')","metadata":{"_uuid":"c99bf94c-c49b-4c54-b3b0-ea42306ceffc","_cell_guid":"51f81fa1-5179-4bf6-be48-9f7d15958ca3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-10T05:47:29.767128Z","iopub.execute_input":"2025-09-10T05:47:29.767414Z","iopub.status.idle":"2025-09-10T05:47:30.330063Z","shell.execute_reply.started":"2025-09-10T05:47:29.767393Z","shell.execute_reply":"2025-09-10T05:47:30.329157Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CHECKPOINT_DIR = './checkpoints'\n\nfiles = os.listdir(CHECKPOINT_DIR)\nfor f in sorted(files):\n    print(f)\nfor f in sorted(files):\n    path = os.path.join(CHECKPOINT_DIR, f)\n    size_mb = os.path.getsize(path) / (1024*1024)\n    print(f'{f} → {size_mb:.2f} MB')","metadata":{"_uuid":"b9ac763f-8d61-4080-8fbb-dbd173587f6c","_cell_guid":"d159cb98-96ae-4bf7-9ccc-8ba0139eb43c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-08T07:52:40.092811Z","iopub.execute_input":"2025-09-08T07:52:40.093061Z","iopub.status.idle":"2025-09-08T07:52:40.098165Z","shell.execute_reply.started":"2025-09-08T07:52:40.093044Z","shell.execute_reply":"2025-09-08T07:52:40.097456Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\n\nCHECKPOINT_DIR = './checkpoints'\n\n\nshutil.rmtree(CHECKPOINT_DIR)\n\n# Recreate the folder for new training\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)","metadata":{"_uuid":"bdf4e99f-2ef3-4e59-8c23-106b66ead682","_cell_guid":"381ee3ce-c457-4def-9f8a-049bb8a00b39","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-08T07:52:35.840242Z","iopub.execute_input":"2025-09-08T07:52:35.841124Z","iopub.status.idle":"2025-09-08T07:52:36.857884Z","shell.execute_reply.started":"2025-09-08T07:52:35.841098Z","shell.execute_reply":"2025-09-08T07:52:36.857264Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\nprompt = 'Cars need newer'\ninput_ids = torch.tensor(tokenizer.encode(prompt).ids, dtype=torch.long, device=DEVICE).unsqueeze(0)\nfor _ in range(50):\n    logits = model(input_ids)\n    next_id = torch.multinomial(F.softmax(logits[:,-1,:], dim=-1),1)\n    input_ids = torch.cat([input_ids, next_id], dim=1)\ngenerated = tokenizer.decode(input_ids.squeeze().tolist())\nprint(f\"Response: {generated}\\n\")","metadata":{"_uuid":"0c8ea394-cf4b-4785-ae28-9c08e4638889","_cell_guid":"fce62b36-aa56-4ed6-bac3-7f9b9c475da0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-09T09:10:24.303800Z","iopub.execute_input":"2025-09-09T09:10:24.304282Z","iopub.status.idle":"2025-09-09T09:10:24.790127Z","shell.execute_reply.started":"2025-09-09T09:10:24.304259Z","shell.execute_reply":"2025-09-09T09:10:24.789374Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#reload model","metadata":{"_uuid":"0ee8a918-2758-4b88-a33d-ade71923d211","_cell_guid":"5a3f713d-5406-4934-92f0-8613b802edd8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport os\nclass GPTConfig:\n    def __init__(self, vocab_size=40000, d_model=512, n_heads=8, n_layers=12, d_ff=4096, max_len=512, dropout=0.05):\n        self.vocab_size = vocab_size\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.n_layers = n_layers\n        self.d_ff = d_ff\n        self.max_len = max_len\n        self.dropout = dropout\n\nCONFIG = GPTConfig()\nSEQ_LEN = CONFIG.max_len\nBATCH_SIZE = 8\nGRAD_ACCUM = 4\nEPOCHS = 1\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nCHECKPOINT_DIR = './checkpoints'\nCHECKPOINT_STEPS = 12000\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)","metadata":{"_uuid":"ba71c22c-e7aa-4961-8352-52ebeb3e3723","_cell_guid":"ea6de10b-85d4-4dd8-bb69-1e71605afa08","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-09T14:02:07.395505Z","iopub.execute_input":"2025-09-09T14:02:07.396047Z","iopub.status.idle":"2025-09-09T14:02:07.401520Z","shell.execute_reply.started":"2025-09-09T14:02:07.396014Z","shell.execute_reply":"2025-09-09T14:02:07.400974Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CausalSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        assert config.d_model % config.n_heads == 0\n        self.n_heads = config.n_heads\n        self.head_dim = config.d_model // config.n_heads\n        self.qkv_proj = nn.Linear(config.d_model, 3*config.d_model)\n        self.out_proj = nn.Linear(config.d_model, config.d_model)\n        self.dropout = nn.Dropout(config.dropout)\n        self.scale = 1/math.sqrt(self.head_dim)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        qkv = self.qkv_proj(x).view(B,T,3,self.n_heads, self.head_dim).permute(2,0,3,1,4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        attn = torch.matmul(q,k.transpose(-2, -1))*self.scale\n        mask = torch.tril(torch.ones(T, T,device=x.device)).unsqueeze(0).unsqueeze(0)\n        attn = attn.masked_fill(mask==0, -torch.inf)\n        attn = torch.softmax(attn, dim=-1)\n        attn = self.dropout(attn)\n        out = torch.matmul(attn, v).transpose(1, 2).contiguous().view(B, T, C)\n        return self.out_proj(out)","metadata":{"_uuid":"4b4ab184-fae9-49cc-9d50-5cdb1a5d4900","_cell_guid":"8a7f82a1-a668-454c-b0d9-14f2ae9e0e77","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-09T14:02:47.968267Z","iopub.execute_input":"2025-09-09T14:02:47.968526Z","iopub.status.idle":"2025-09-09T14:02:47.975962Z","shell.execute_reply.started":"2025-09-09T14:02:47.968506Z","shell.execute_reply":"2025-09-09T14:02:47.975101Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(config.d_model)\n        self.attn = CausalSelfAttention(config)\n        self.ln2 = nn.LayerNorm(config.d_model)\n        self.ff = nn.Sequential(\n            nn.Linear(config.d_model, config.d_ff),\n            nn.GELU(),\n            nn.Linear(config.d_ff, config.d_model),\n            nn.Dropout(config.dropout)\n        )\n\n    def forward(self, x):\n        x = x + self.attn(self.ln1(x))\n        x = x + self.ff(self.ln2(x))\n        return x","metadata":{"_uuid":"7e6bb02d-0c8c-43e7-af0f-ca9d96e10351","_cell_guid":"cd22e35b-a6bd-42ba-b559-a02c25f2abe6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-09T14:02:10.580576Z","iopub.execute_input":"2025-09-09T14:02:10.580924Z","iopub.status.idle":"2025-09-09T14:02:10.585860Z","shell.execute_reply.started":"2025-09-09T14:02:10.580900Z","shell.execute_reply":"2025-09-09T14:02:10.585114Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.tok_emb = nn.Embedding(config.vocab_size, config.d_model)\n        self.pos_emb = nn.Embedding(config.max_len, config.d_model)\n        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(config.n_layers)])\n        self.ln_f = nn.LayerNorm(config.d_model)\n        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n\n    def forward(self, idx):\n        B, T = idx.shape\n        tok = self.tok_emb(idx)\n        pos = self.pos_emb(torch.arange(T, device=idx.device)).unsqueeze(0).expand(B, -1, -1)\n        x = tok+pos\n        for block in self.blocks:\n            x = block(x)\n        x = self.ln_f(x)\n        return self.head(x)\n\nmodel = GPT(CONFIG).to(DEVICE)\nprint(f\"trainabale model params: {sum(p.numel() for p in model.parameters() if p.requires_grad)//1e6:.1f}M\")\nprint(f\"total model params: {sum(p.numel() for p in model.parameters())//1e6:.1f}M\")","metadata":{"_uuid":"2ba191e9-4894-4df6-8d12-6c431df40cd8","_cell_guid":"fb3546f3-4de5-45c3-ad0d-40ede5bbf594","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-09T14:03:26.398534Z","iopub.execute_input":"2025-09-09T14:03:26.398987Z","iopub.status.idle":"2025-09-09T14:03:27.629976Z","shell.execute_reply.started":"2025-09-09T14:03:26.398964Z","shell.execute_reply":"2025-09-09T14:03:27.629370Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = GPT()\nmodel.load_state_dict(path, map_location='cuda')\nmodel.eval()","metadata":{"_uuid":"c8253b4c-9f9e-45fe-b9be-71891de96080","_cell_guid":"26d9c598-58eb-4b7e-8f16-20c53f473c17","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}